{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia3SX5dN-LyA"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2GBsmj-FvaN3"
   },
   "outputs": [],
   "source": [
    "#Step 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ1Fx6xe-P0D"
   },
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1x7eNGKc9R0F"
   },
   "outputs": [],
   "source": [
    "#Step 3: Dataset Class (Brain Tumor Segmentation)\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class BrainTumorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images = sorted([os.path.join(images_dir, f) for f in os.listdir(images_dir)])\n",
    "        self.masks  = sorted([os.path.join(masks_dir, f) for f in os.listdir(masks_dir)])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        mask  = Image.open(self.masks[idx]).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask  = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xqBq5MAy9VG2"
   },
   "outputs": [],
   "source": [
    "#Step 4: Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHQXeSmq-a0p"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vv99SK_t9hqI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6292, Val: 1574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_images = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/train/images\"\n",
    "train_masks  = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/train/masks\"\n",
    "\n",
    "dataset = BrainTumorDataset(train_images, train_masks, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq0Kq28n-jds"
   },
   "source": [
    "# Swin-Unet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FgyjHwnNvy0L"
   },
   "outputs": [],
   "source": [
    "#Step 5: Swin U-Net Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\", # Swin Transformer\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPurZYHo-pMN"
   },
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8snjQNg99ry-"
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = smp.losses.DiceLoss(mode='binary')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYLEZP6-sa-"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WWSI7yux9t9_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:22<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.3611 | Val Loss: 0.2464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Train Loss: 0.2511 | Val Loss: 0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Train Loss: 0.2221 | Val Loss: 0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Train Loss: 0.2047 | Val Loss: 0.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Train Loss: 0.2031 | Val Loss: 0.2335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Train Loss: 0.1916 | Val Loss: 0.2295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:22<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Train Loss: 0.1861 | Val Loss: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Train Loss: 0.1759 | Val Loss: 0.2144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Train Loss: 0.1680 | Val Loss: 0.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Train Loss: 0.1570 | Val Loss: 0.1592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Train Loss: 0.1572 | Val Loss: 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Train Loss: 0.1587 | Val Loss: 0.1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Train Loss: 0.1429 | Val Loss: 0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Train Loss: 0.1420 | Val Loss: 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Train Loss: 0.1536 | Val Loss: 0.1573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Train Loss: 0.1457 | Val Loss: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Train Loss: 0.1271 | Val Loss: 0.1505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Train Loss: 0.1356 | Val Loss: 0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Train Loss: 0.1288 | Val Loss: 0.1424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1573/1573 [01:21<00:00, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Train Loss: 0.1218 | Val Loss: 0.1429\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Training Loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "arr_loss = []\n",
    "arr_val_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        preds = model(images)\n",
    "        loss = loss_fn(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    arr_loss.append(train_loss)\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            preds = model(images)\n",
    "            loss = loss_fn(preds, masks)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"./best_swinunet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.361053874404735, 0.25110906032870245, 0.222121250758259, 0.20470547463889344, 0.20305545233043226, 0.1915684346173572, 0.18609065154670382, 0.17589696766234322, 0.16803134829090116, 0.15703129582699135, 0.1572187702821003, 0.15868924172608168, 0.14285741315400305, 0.14202350139314782, 0.15360473805904692, 0.14567229111938815, 0.12714307723497026, 0.13559026784072314, 0.12880292172956376, 0.12182841358560706]\n",
      "[0.24637850090331836, 0.22649635094676526, 0.2589454066934924, 0.28340094010842026, 0.2335007553778324, 0.22945683377648368, 0.22019361118374742, 0.21442013646140318, 0.1679384458791181, 0.15922598260913404, 0.17739438072679006, 0.15812735796579855, 0.17250827333043675, 0.15649576371696394, 0.15734674800471002, 0.15112149896960572, 0.15053618181175388, 0.17245382963098246, 0.14239043284793795, 0.14292416188317508]\n"
     ]
    }
   ],
   "source": [
    "print(arr_loss)\n",
    "print(arr_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255967 4679 1498 0\n",
      "257364 2208 2572 0\n",
      "253916 2142 2753 3333\n",
      "257099 517 2740 1788\n",
      "257600 2975 1569 0\n",
      "256514 502 725 4403\n",
      "254981 6151 1012 0\n",
      "258171 2160 144 1669\n",
      "243861 8105 10178 0\n",
      "258856 2120 1163 5\n",
      "259323 2384 425 12\n",
      "254931 5024 2107 82\n",
      "257489 450 497 3708\n",
      "251130 1028 1438 8548\n",
      "253256 5973 179 2736\n",
      "256178 2978 1611 1377\n",
      "260155 434 713 842\n",
      "252146 1499 3645 4854\n",
      "250252 582 2586 8724\n",
      "256679 442 815 4208\n",
      "258225 1017 418 2484\n",
      "256304 781 942 4117\n",
      "258105 518 967 2554\n",
      "256000 328 765 5051\n",
      "259556 1410 216 962\n",
      "251371 4561 3934 2278\n",
      "253431 669 894 7150\n",
      "255069 1698 460 4917\n",
      "256156 1126 732 4130\n",
      "260308 191 461 1184\n",
      "251557 3120 740 6727\n",
      "252090 1178 4741 4135\n",
      "259109 267 566 2202\n",
      "256796 2739 1013 1596\n",
      "258057 500 876 2711\n",
      "259977 328 332 1507\n",
      "246572 226 13937 1409\n",
      "254414 698 824 6208\n",
      "256282 667 1122 4073\n",
      "254055 378 3588 4123\n",
      "253412 1790 6781 161\n",
      "249537 5107 7500 0\n",
      "246058 5500 1430 9156\n",
      "258383 574 338 2849\n",
      "244755 3913 13476 0\n",
      "245594 3636 3533 9381\n",
      "249093 9908 3116 27\n",
      "252560 6923 363 2298\n",
      "256357 3716 213 1858\n",
      "259257 1977 232 678\n",
      "246345 5333 10193 273\n",
      "252818 1775 3036 4515\n",
      "257889 4070 12 173\n",
      "251934 2540 4455 3215\n",
      "252567 2348 397 6832\n",
      "257463 3165 354 1162\n",
      "260634 825 448 237\n",
      "259526 901 1678 39\n",
      "254590 1489 6065 0\n",
      "257535 1474 2589 546\n",
      "257733 1488 1204 1719\n",
      "252813 2078 2563 4690\n",
      "249796 1607 1847 8894\n",
      "257064 468 566 4046\n",
      "260422 200 572 950\n",
      "254495 577 1495 5577\n",
      "257826 4122 5 191\n",
      "260136 329 241 1438\n",
      "257098 2363 935 1748\n",
      "257525 3407 82 1130\n",
      "258754 385 429 2576\n",
      "243067 1271 2364 15442\n",
      "238100 1608 2157 20279\n",
      "258986 419 483 2256\n",
      "259171 281 413 2279\n",
      "259640 246 695 1563\n",
      "259112 551 737 1744\n",
      "257811 369 631 3333\n",
      "257195 169 1459 3321\n",
      "249636 82 4058 8368\n",
      "259871 480 233 1560\n",
      "257230 2813 98 2003\n",
      "251589 7923 836 1796\n",
      "245118 2198 14828 0\n",
      "259077 2639 144 284\n",
      "258435 422 471 2816\n",
      "257986 95 1616 2447\n",
      "252237 3209 6698 0\n",
      "257953 692 469 3030\n",
      "258780 749 288 2327\n",
      "252927 727 707 7783\n",
      "260308 191 461 1184\n",
      "259059 628 536 1921\n",
      "260284 705 235 920\n",
      "258605 605 664 2270\n",
      "254863 5892 1388 1\n",
      "243480 21 12387 6256\n",
      "259852 622 205 1465\n",
      "259703 2091 350 0\n",
      "259433 863 607 1241\n",
      "259933 624 277 1310\n",
      "257041 424 663 4016\n",
      "254008 616 1000 6520\n",
      "256034 778 1317 4015\n",
      "259696 909 375 1164\n",
      "251549 1107 1245 8243\n",
      "250427 1091 1857 8769\n",
      "257650 502 501 3491\n",
      "258375 416 494 2859\n",
      "255801 718 441 5184\n",
      "261007 183 274 680\n",
      "254715 6533 41 855\n",
      "255020 4092 104 2928\n",
      "253202 432 2271 6239\n",
      "259927 441 254 1522\n",
      "259780 989 191 1184\n",
      "258703 681 225 2535\n",
      "258735 625 625 2159\n",
      "256821 1015 948 3360\n",
      "259408 637 295 1804\n",
      "253506 337 1643 6658\n",
      "258145 1282 412 2305\n",
      "255042 3965 1570 1567\n",
      "258886 271 1010 1977\n",
      "256317 3593 916 1318\n",
      "259329 285 839 1691\n",
      "259047 334 540 2223\n",
      "259778 428 723 1215\n",
      "258957 424 372 2391\n",
      "254568 223 2642 4711\n",
      "256206 3430 2164 344\n",
      "253472 5704 2410 558\n",
      "259022 357 376 2389\n",
      "248903 250 4560 8431\n",
      "256700 2827 2617 0\n",
      "257392 847 2512 1393\n",
      "256754 2214 2774 402\n",
      "254491 514 1219 5920\n",
      "258861 1316 334 1633\n",
      "260174 1044 923 3\n",
      "255749 2833 1904 1658\n",
      "261230 251 214 449\n",
      "258252 1513 1077 1302\n",
      "261200 549 172 223\n",
      "257843 1327 816 2158\n",
      "261098 343 222 481\n",
      "258282 271 728 2863\n",
      "259686 236 444 1778\n",
      "260242 271 405 1226\n",
      "258425 1642 506 1571\n",
      "257989 726 957 2472\n",
      "253979 644 886 6635\n",
      "257435 275 760 3674\n",
      "256479 19 3692 1954\n",
      "243894 247 5155 12848\n",
      "252467 1878 2922 4877\n",
      "245531 306 5497 10810\n",
      "247788 2238 5459 6659\n",
      "256221 730 1392 3801\n",
      "253879 665 3062 4538\n",
      "251109 981 1961 8093\n",
      "257744 317 2562 1521\n",
      "255014 4446 213 2471\n",
      "250037 74 3766 8267\n",
      "249702 131 3602 8709\n",
      "257395 142 1589 3018\n",
      "258391 438 557 2758\n",
      "248217 764 4668 8495\n",
      "248158 1416 3580 8990\n",
      "250419 3436 2136 6153\n",
      "253168 1404 2154 5418\n",
      "254303 741 1842 5258\n",
      "261122 168 309 545\n",
      "260719 239 241 945\n",
      "258750 607 698 2089\n",
      "253958 6551 1635 0\n",
      "252610 2719 258 6557\n",
      "255945 1059 2316 2824\n",
      "258341 2545 156 1102\n",
      "260349 599 436 760\n",
      "260457 316 258 1113\n",
      "258548 1352 840 1404\n",
      "254209 1681 4393 1861\n",
      "251078 537 1078 9451\n",
      "254421 737 819 6167\n",
      "255097 629 6406 12\n",
      "255448 363 4149 2184\n",
      "258621 1396 1476 651\n",
      "249198 735 1189 11022\n",
      "247814 975 1178 12177\n",
      "259105 2774 44 221\n",
      "254335 452 4063 3294\n",
      "233231 1153 26322 1438\n",
      "260482 334 312 1016\n",
      "257995 2807 114 1228\n",
      "256303 760 565 4516\n",
      "259466 431 316 1931\n",
      "250909 776 2092 8367\n",
      "251075 655 2184 8230\n",
      "257386 467 1208 3083\n",
      "257948 519 538 3139\n",
      "259234 846 220 1844\n",
      "254847 640 1101 5556\n",
      "257864 2049 947 1284\n",
      "256306 428 1616 3794\n",
      "252912 907 1981 6344\n",
      "256768 2974 2402 0\n",
      "251404 902 4252 5586\n",
      "252755 685 2379 6325\n",
      "246414 3185 11545 1000\n",
      "251186 925 7352 2681\n",
      "261111 195 154 684\n",
      "260815 321 216 792\n",
      "258323 303 456 3062\n",
      "258440 377 426 2901\n",
      "250074 2747 3542 5781\n",
      "248089 8927 5128 0\n",
      "251920 3700 2336 4188\n",
      "258826 719 266 2333\n",
      "253183 749 2402 5810\n",
      "256420 607 468 4649\n",
      "244821 551 4424 12348\n",
      "252551 7514 234 1845\n",
      "261611 210 76 247\n",
      "260879 243 252 770\n",
      "258779 298 502 2565\n",
      "260225 798 131 990\n",
      "260426 270 449 999\n",
      "260180 283 450 1231\n",
      "256063 1131 895 4055\n",
      "256368 705 888 4183\n",
      "259630 608 409 1497\n",
      "258734 399 395 2616\n",
      "258930 458 399 2357\n",
      "257787 1990 652 1715\n",
      "256640 292 888 4324\n",
      "255450 4401 2293 0\n",
      "240131 409 9629 11975\n",
      "239591 0 15909 6644\n",
      "258310 463 675 2696\n",
      "256906 277 1598 3363\n",
      "258782 1111 365 1886\n",
      "254508 1129 1372 5135\n",
      "252909 757 913 7565\n",
      "251593 799 1029 8723\n",
      "251427 812 1108 8797\n",
      "256605 350 797 4392\n",
      "259856 207 430 1651\n",
      "259843 416 386 1499\n",
      "258858 371 437 2478\n",
      "251742 241 3734 6427\n",
      "246005 672 1893 13574\n",
      "256477 828 3626 1213\n",
      "252550 499 4656 4439\n",
      "246072 2620 2418 11034\n",
      "249834 1432 6991 3887\n",
      "247788 991 1398 11967\n",
      "246503 5770 1696 8175\n",
      "236883 1559 10641 13061\n",
      "248716 1464 4889 7075\n",
      "246818 3090 1344 10892\n",
      "251445 1312 1052 8335\n",
      "253337 1792 2720 4295\n",
      "238814 823 3518 18989\n",
      "245533 1204 2305 13102\n",
      "249626 738 1489 10291\n",
      "247618 852 1601 12073\n",
      "229742 1321 14472 16609\n",
      "248535 1562 2099 9948\n",
      "254239 1290 1385 5230\n",
      "235374 1393 6031 19346\n",
      "231647 1267 2432 26798\n",
      "245814 1301 2430 12599\n",
      "247216 1675 2616 10637\n",
      "251354 1032 1143 8615\n",
      "260167 797 215 965\n",
      "254090 955 1026 6073\n",
      "250565 2198 1462 7919\n",
      "237329 4445 4713 15657\n",
      "216343 1115 29200 15486\n",
      "251961 2280 7903 0\n",
      "250363 1623 3171 6987\n",
      "249718 1621 1929 8876\n",
      "258190 836 772 2346\n",
      "244534 2769 2842 11999\n",
      "252857 1266 2530 5491\n",
      "232299 1803 1795 26247\n",
      "252750 1223 1561 6610\n",
      "251701 706 2671 7066\n",
      "232225 2937 6526 20456\n",
      "246083 2491 1556 12014\n",
      "236666 4601 5276 15601\n",
      "255540 1305 1751 3548\n",
      "250937 1129 2496 7582\n",
      "245781 1786 3861 10716\n",
      "252394 2020 2182 5548\n",
      "241803 2740 3578 14023\n",
      "236759 3116 4893 17376\n",
      "238748 1272 2511 19613\n",
      "248075 2332 4687 7050\n",
      "251163 2543 1217 7221\n",
      "259742 1097 595 710\n",
      "254087 1541 2176 4340\n",
      "229533 3561 4415 24635\n",
      "244312 2660 3278 11894\n",
      "239387 2586 4608 15563\n",
      "237227 2147 5799 16971\n",
      "240628 4774 3598 13144\n",
      "257438 692 1821 2193\n",
      "244665 2989 2738 11752\n",
      "255275 996 2833 3040\n",
      "250838 1290 2272 7744\n",
      "233760 2298 7966 18120\n",
      "245606 5476 2040 9022\n",
      "256811 1138 2098 2097\n",
      "245780 1996 3403 10965\n",
      "253933 1999 2941 3271\n",
      "246096 2408 2920 10720\n",
      "255916 1546 1890 2792\n",
      "253566 1573 2407 4598\n",
      "243548 1638 6498 10460\n",
      "252537 1811 2252 5544\n",
      "250550 1561 1676 8357\n",
      "259358 1044 248 1494\n",
      "258482 451 483 2728\n",
      "253278 832 1293 6741\n",
      "259517 802 237 1588\n",
      "257937 389 523 3295\n",
      "260804 308 431 601\n",
      "258338 328 597 2881\n",
      "259901 329 336 1578\n",
      "256663 463 683 4335\n",
      "257260 419 586 3879\n",
      "259343 451 364 1986\n",
      "260111 280 225 1528\n",
      "246147 591 2109 13297\n",
      "258747 409 632 2356\n",
      "249054 740 1121 11229\n",
      "254341 556 1164 6083\n",
      "261340 184 183 437\n",
      "256552 536 481 4575\n",
      "250638 659 797 10050\n",
      "260015 334 407 1388\n",
      "256624 439 777 4304\n",
      "258536 411 1684 1513\n",
      "256722 370 554 4498\n",
      "252035 583 1019 8507\n",
      "250742 837 1500 9065\n",
      "254395 1439 2430 3880\n",
      "256017 1645 541 3941\n",
      "252080 412 1975 7677\n",
      "257193 1082 1716 2153\n",
      "257598 353 562 3631\n",
      "256013 1279 1793 3059\n",
      "256724 440 980 4000\n",
      "247842 859 740 12703\n",
      "259385 292 325 2142\n",
      "258221 415 457 3051\n",
      "252188 804 726 8426\n",
      "255565 1020 427 5132\n",
      "258216 428 834 2666\n",
      "258889 1326 278 1651\n",
      "259831 283 440 1590\n",
      "258988 541 371 2244\n",
      "259983 547 224 1390\n",
      "251881 652 1297 8314\n",
      "259207 389 455 2093\n",
      "256192 767 493 4692\n",
      "259794 683 201 1466\n",
      "256213 585 490 4856\n",
      "259194 540 285 2125\n",
      "260866 181 303 794\n",
      "257029 415 550 4150\n",
      "257685 824 358 3277\n",
      "259205 443 416 2080\n",
      "260715 345 269 815\n",
      "250930 741 830 9643\n",
      "258305 573 983 2283\n",
      "247544 868 1717 12015\n",
      "256742 355 861 4186\n",
      "256253 469 951 4471\n",
      "258756 529 426 2433\n",
      "259833 247 475 1589\n",
      "259020 386 324 2414\n",
      "254856 795 628 5865\n",
      "258960 311 576 2297\n",
      "253648 546 830 7120\n",
      "256928 585 442 4189\n",
      "258032 340 461 3311\n",
      "259915 522 334 1373\n",
      "258036 552 339 3217\n",
      "247750 2813 4025 7556\n",
      "251956 1354 5608 3226\n",
      "247918 1640 2551 10035\n",
      "253259 612 1384 6889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242393 6126 3459 10166\n",
      "244539 1554 3618 12433\n",
      "240832 1158 4320 15834\n",
      "233743 2367 8952 17082\n",
      "240173 1136 3480 17355\n",
      "243015 5795 2790 10544\n",
      "248111 1004 3215 9814\n",
      "256295 1180 1130 3539\n",
      "253537 1069 1861 5677\n",
      "257604 783 707 3050\n",
      "253494 5350 769 2531\n",
      "253437 959 1115 6633\n",
      "247949 1236 3404 9555\n",
      "248743 1396 1992 10013\n",
      "232565 1888 16598 11093\n",
      "253750 2364 1773 4257\n",
      "257157 1333 1952 1702\n",
      "246523 499 2874 12248\n",
      "238407 1516 5338 16883\n",
      "246234 1520 2253 12137\n",
      "246271 1460 2216 12197\n",
      "241269 1434 2491 16950\n",
      "250427 5746 2213 3758\n",
      "240622 2423 4315 14784\n",
      "247020 1925 3237 9962\n",
      "248794 1672 4869 6809\n",
      "253028 634 590 7892\n",
      "259542 1013 205 1384\n",
      "256973 505 849 3817\n",
      "254981 492 586 6085\n",
      "258927 761 349 2107\n",
      "256258 896 468 4522\n",
      "258441 502 830 2371\n",
      "258216 428 834 2666\n",
      "257526 477 372 3769\n",
      "260012 327 313 1492\n",
      "259342 495 447 1860\n",
      "260450 293 297 1104\n",
      "258397 335 460 2952\n",
      "259584 572 357 1631\n",
      "257736 401 665 3342\n",
      "259168 887 179 1910\n",
      "255672 1149 399 4924\n",
      "256091 601 673 4779\n",
      "261044 188 267 645\n",
      "258079 591 369 3105\n",
      "255933 464 1040 4707\n",
      "258967 306 388 2483\n",
      "260766 189 340 849\n",
      "249868 756 847 10673\n",
      "255111 497 1645 4891\n",
      "254974 571 654 5945\n",
      "258060 474 463 3147\n",
      "258493 1160 387 2104\n",
      "254432 601 792 6319\n",
      "254968 871 534 5771\n",
      "260389 252 278 1225\n",
      "257778 583 423 3360\n",
      "257371 643 869 3261\n",
      "257333 1779 581 2451\n",
      "260473 367 228 1076\n",
      "259891 265 342 1646\n",
      "257291 548 505 3800\n",
      "256891 1155 224 3874\n",
      "260385 256 432 1071\n",
      "251452 744 722 9226\n",
      "252243 519 1381 8001\n",
      "258853 248 636 2407\n",
      "251968 713 784 8679\n",
      "254401 705 662 6376\n",
      "260454 370 226 1094\n",
      "249217 501 4788 7638\n",
      "251985 655 715 8789\n",
      "257079 369 464 4232\n",
      "260102 448 260 1334\n",
      "258471 301 2491 881\n",
      "255473 440 755 5476\n",
      "252331 765 641 8407\n",
      "254805 728 515 6096\n",
      "259765 294 336 1749\n",
      "258951 306 318 2569\n",
      "253287 769 633 7455\n",
      "251552 1105 1274 8213\n",
      "250410 1864 1875 7995\n",
      "244917 2024 3857 11346\n",
      "252154 1118 1710 7162\n",
      "255677 1169 1745 3553\n",
      "258037 1910 482 1715\n",
      "254687 1512 1489 4456\n",
      "256299 564 1165 4116\n",
      "252040 1136 1738 7230\n",
      "256195 1735 1185 3029\n",
      "247565 2136 2732 9711\n",
      "256177 712 815 4440\n",
      "253393 1912 2265 4574\n",
      "252238 1130 847 7929\n",
      "248047 2126 2917 9054\n",
      "238803 1919 4982 16440\n",
      "256233 583 504 4824\n",
      "255200 1463 686 4795\n",
      "254879 1143 803 5319\n",
      "257723 419 518 3484\n",
      "252598 551 663 8332\n",
      "259174 327 554 2089\n",
      "260524 511 295 814\n",
      "257121 567 427 4029\n",
      "260303 378 152 1311\n",
      "260160 399 226 1359\n",
      "260636 254 232 1022\n",
      "257013 1005 396 3730\n",
      "258264 737 329 2814\n",
      "257351 1723 361 2709\n",
      "257835 1029 364 2916\n",
      "257953 324 712 3155\n",
      "253885 791 477 6991\n",
      "255871 572 489 5212\n",
      "258788 638 338 2380\n",
      "260873 360 187 724\n",
      "257428 411 360 3945\n",
      "255662 500 486 5496\n",
      "257578 427 520 3619\n",
      "259422 600 543 1579\n",
      "249999 713 876 10556\n",
      "256050 431 735 4928\n",
      "257969 383 654 3138\n",
      "260954 412 235 543\n",
      "259445 298 541 1860\n",
      "259765 659 211 1509\n",
      "251720 1926 531 7967\n",
      "251593 341 2013 8197\n",
      "258390 1898 286 1570\n",
      "260263 218 428 1235\n",
      "256702 692 863 3887\n",
      "256802 376 613 4353\n",
      "258971 361 367 2445\n",
      "257178 411 603 3952\n",
      "256582 681 506 4375\n",
      "258336 621 370 2817\n",
      "256364 658 403 4719\n",
      "259144 556 244 2200\n",
      "249428 815 1070 10831\n",
      "249784 679 1252 10429\n",
      "258290 551 457 2846\n",
      "253785 825 890 6644\n",
      "246215 876 906 14147\n",
      "254336 1455 940 5413\n",
      "260334 210 341 1259\n",
      "255652 1254 302 4936\n",
      "260550 336 253 1005\n",
      "255680 1012 514 4938\n",
      "254540 818 594 6192\n",
      "253379 682 580 7503\n",
      "256568 738 469 4369\n",
      "256235 764 429 4716\n",
      "258710 467 597 2370\n",
      "256974 565 427 4178\n",
      "259574 1039 264 1267\n",
      "256712 648 584 4200\n",
      "251663 621 726 9134\n",
      "255019 686 694 5745\n",
      "258634 540 344 2626\n",
      "259823 384 237 1700\n",
      "261270 280 177 417\n",
      "259268 1008 176 1692\n",
      "254229 516 809 6590\n",
      "259540 863 362 1379\n",
      "260163 294 211 1476\n",
      "261659 143 93 249\n",
      "259829 887 268 1160\n",
      "259464 1427 136 1117\n",
      "261184 256 178 526\n",
      "258014 1724 277 2129\n",
      "260259 284 246 1355\n",
      "258684 437 446 2577\n",
      "258821 1370 124 1829\n",
      "261123 202 187 632\n",
      "259876 1125 126 1017\n",
      "259930 1077 186 951\n",
      "260163 360 248 1373\n",
      "260471 238 276 1159\n",
      "260845 867 90 342\n",
      "256552 1102 263 4227\n",
      "254521 590 532 6501\n",
      "257245 1421 174 3304\n",
      "260163 326 266 1389\n",
      "260670 390 195 889\n",
      "258217 1239 934 1754\n",
      "259779 1649 40 676\n",
      "259456 665 439 1584\n",
      "260137 474 285 1248\n",
      "259471 379 375 1919\n",
      "259508 448 826 1362\n",
      "260933 247 288 676\n",
      "260626 290 277 951\n",
      "259455 1523 154 1012\n",
      "259203 1802 21 1118\n",
      "260662 298 220 964\n",
      "260413 278 299 1154\n",
      "258115 396 537 3096\n",
      "258067 289 790 2998\n",
      "260240 234 291 1379\n",
      "259182 575 460 1927\n",
      "257352 550 611 3631\n",
      "259222 1215 161 1546\n",
      "257903 664 513 3064\n",
      "259331 1499 212 1102\n",
      "260118 348 354 1324\n",
      "257926 607 318 3293\n",
      "258244 1813 135 1952\n",
      "260101 304 369 1370\n",
      "258604 1630 186 1724\n",
      "259936 1749 0 459\n",
      "258960 1583 659 942\n",
      "259844 368 324 1608\n",
      "260007 546 191 1400\n",
      "258014 432 596 3102\n",
      "260128 439 269 1308\n",
      "260263 492 305 1084\n",
      "259229 1903 39 973\n",
      "256133 1075 813 4123\n",
      "257495 2131 140 2378\n",
      "257942 382 391 3429\n",
      "259184 1251 185 1524\n",
      "260503 251 314 1076\n",
      "257843 476 399 3426\n",
      "258897 1418 115 1714\n",
      "259728 535 276 1605\n",
      "260207 292 308 1337\n",
      "258919 1335 175 1715\n",
      "259914 1757 3 470\n",
      "259784 1128 813 419\n",
      "259949 1361 34 800\n",
      "259578 1223 68 1275\n",
      "258776 874 588 1906\n",
      "261187 278 156 523\n",
      "260119 953 239 833\n",
      "258795 603 640 2106\n",
      "256899 1128 826 3291\n",
      "260383 284 222 1255\n",
      "260179 393 230 1342\n",
      "257964 418 522 3240\n",
      "258978 1974 111 1081\n",
      "259673 1014 340 1117\n",
      "259229 1252 642 1021\n",
      "259457 711 396 1580\n",
      "259167 1320 144 1513\n",
      "260630 456 179 879\n",
      "259474 1912 254 504\n",
      "258204 1597 286 2057\n",
      "260423 242 252 1227\n",
      "259939 314 285 1606\n",
      "258614 1303 152 2075\n",
      "261077 225 181 661\n",
      "261371 307 124 342\n",
      "260518 360 156 1110\n",
      "260124 349 258 1413\n",
      "259824 446 309 1565\n",
      "258792 2397 94 861\n",
      "260376 602 135 1031\n",
      "261300 221 155 468\n",
      "259072 477 271 2324\n",
      "256432 1107 300 4305\n",
      "254404 473 645 6622\n",
      "259478 1022 160 1484\n",
      "257434 1925 177 2608\n",
      "257648 840 233 3423\n",
      "259025 2022 90 1007\n",
      "260057 260 345 1482\n",
      "261227 157 253 507\n",
      "260105 1192 59 788\n",
      "260840 431 100 773\n",
      "259334 550 421 1839\n",
      "260042 269 396 1437\n",
      "256852 945 543 3804\n",
      "259499 379 316 1950\n",
      "258476 1584 335 1749\n",
      "260961 250 237 696\n",
      "260150 93 1477 424\n",
      "260698 348 192 906\n",
      "259865 1737 81 461\n",
      "259100 1200 114 1730\n",
      "260584 255 305 1000\n",
      "260476 245 264 1159\n",
      "257597 1343 290 2914\n",
      "258686 2684 228 546\n",
      "259062 296 271 2515\n",
      "259974 277 239 1654\n",
      "260304 290 315 1235\n",
      "260158 418 406 1162\n",
      "258050 438 664 2992\n",
      "258980 730 361 2073\n",
      "260402 358 282 1102\n",
      "255181 1020 954 4989\n",
      "257676 835 459 3174\n",
      "260653 373 379 739\n",
      "260781 278 248 837\n",
      "259623 709 283 1529\n",
      "259453 572 305 1814\n",
      "256314 811 830 4189\n",
      "258152 1724 542 1726\n",
      "256425 1367 848 3504\n",
      "258608 229 2004 1303\n",
      "260954 991 44 155\n",
      "256899 2829 2416 0\n",
      "257962 687 897 2598\n",
      "257592 445 797 3310\n",
      "258419 1562 2163 0\n",
      "257683 2417 390 1654\n",
      "258946 885 316 1997\n",
      "259264 502 482 1896\n",
      "258769 1272 240 1863\n",
      "258202 2015 259 1668\n",
      "258955 643 574 1972\n",
      "258823 835 453 2033\n",
      "259927 614 304 1299\n",
      "257990 884 565 2705\n",
      "258843 411 554 2336\n",
      "258381 467 472 2824\n",
      "256362 2712 173 2897\n",
      "259911 718 399 1116\n",
      "255996 712 464 4972\n",
      "260359 268 195 1322\n",
      "259771 543 263 1567\n",
      "252931 1790 850 6573\n",
      "257559 484 540 3561\n",
      "257560 503 1519 2562\n",
      "258218 931 657 2338\n",
      "260037 536 424 1147\n",
      "259938 398 544 1264\n",
      "259558 694 313 1579\n",
      "259368 658 314 1804\n",
      "260309 803 186 846\n",
      "259325 673 397 1749\n",
      "257893 671 938 2642\n",
      "256941 598 530 4075\n",
      "254767 613 641 6123\n",
      "256214 1428 550 3952\n",
      "255988 471 2240 3445\n",
      "259739 419 380 1606\n",
      "259122 411 404 2207\n",
      "260861 349 343 591\n",
      "256342 982 586 4234\n",
      "258476 522 891 2255\n",
      "260192 490 463 999\n",
      "259915 400 462 1367\n",
      "248632 612 1247 11653\n",
      "251976 6843 828 2497\n",
      "257864 761 642 2877\n",
      "256326 597 791 4430\n",
      "258388 357 1486 1913\n",
      "260437 1111 596 0\n",
      "259832 279 321 1712\n",
      "260876 758 127 383\n",
      "260289 591 173 1091\n",
      "261094 296 226 528\n",
      "258017 500 547 3080\n",
      "254693 4988 2463 0\n",
      "257045 1066 537 3496\n",
      "254903 623 971 5647\n",
      "260074 319 316 1435\n",
      "257320 388 1871 2565\n",
      "260133 225 543 1243\n",
      "258887 607 439 2211\n",
      "260158 805 235 946\n",
      "259625 739 352 1428\n",
      "258124 354 496 3170\n",
      "260622 260 267 995\n",
      "260598 793 129 624\n",
      "252690 679 722 8053\n",
      "258411 481 1215 2037\n",
      "258699 403 499 2543\n",
      "254507 1367 744 5526\n",
      "256960 684 619 3881\n",
      "260051 657 190 1246\n",
      "257888 1878 2378 0\n",
      "259399 741 1313 691\n",
      "260344 424 175 1201\n",
      "256857 1165 438 3684\n",
      "258071 764 552 2757\n",
      "260444 425 316 959\n",
      "260070 293 379 1402\n",
      "259783 249 472 1640\n",
      "261242 307 180 415\n",
      "260167 691 111 1175\n",
      "258045 479 455 3165\n",
      "257726 548 661 3209\n",
      "256714 550 455 4425\n",
      "259220 451 500 1973\n",
      "260800 255 217 872\n",
      "256288 453 511 4892\n",
      "258318 2479 479 868\n",
      "259273 672 321 1878\n",
      "260116 457 327 1244\n",
      "260624 1004 46 470\n",
      "260107 544 855 638\n",
      "258114 1161 342 2527\n",
      "259256 1122 279 1487\n",
      "260196 653 228 1067\n",
      "258893 375 603 2273\n",
      "260518 409 299 918\n",
      "259871 1768 62 443\n",
      "258236 601 464 2843\n",
      "258014 2782 185 1163\n",
      "260042 502 220 1380\n",
      "260019 972 93 1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260197 357 208 1382\n",
      "258373 335 510 2926\n",
      "259895 381 288 1580\n",
      "260210 616 222 1096\n",
      "260408 245 378 1113\n",
      "260749 240 223 932\n",
      "261050 244 222 628\n",
      "260223 696 101 1124\n",
      "258110 529 383 3122\n",
      "257396 1121 250 3377\n",
      "259249 790 187 1918\n",
      "259960 824 162 1198\n",
      "259679 408 240 1817\n",
      "261131 300 149 564\n",
      "261027 333 200 584\n",
      "261169 312 137 526\n",
      "261048 317 188 591\n",
      "260387 291 270 1196\n",
      "260297 412 275 1160\n",
      "260916 486 169 573\n",
      "258355 766 432 2591\n",
      "257686 1075 310 3073\n",
      "258011 929 401 2803\n",
      "259714 589 222 1619\n",
      "260071 317 305 1451\n",
      "260352 398 246 1148\n",
      "259524 543 253 1824\n",
      "257324 736 1057 3027\n",
      "258774 493 759 2118\n",
      "257484 307 1919 2434\n",
      "260647 193 397 907\n",
      "257959 1036 513 2636\n",
      "248524 705 1498 11417\n",
      "258613 504 466 2561\n",
      "253992 3415 479 4258\n",
      "260609 441 293 801\n",
      "260025 311 291 1517\n",
      "260776 220 251 897\n",
      "261142 201 215 586\n",
      "261347 305 100 392\n",
      "261125 322 136 561\n",
      "260976 694 108 366\n",
      "257425 782 500 3437\n",
      "257712 1202 458 2772\n",
      "256981 362 1234 3567\n",
      "260871 590 142 541\n",
      "260002 975 138 1029\n",
      "260393 1246 132 373\n",
      "261113 175 363 493\n",
      "259301 520 283 2040\n",
      "261033 537 84 490\n",
      "261021 862 53 208\n",
      "258518 326 454 2846\n",
      "260808 273 226 837\n",
      "259411 461 414 1858\n",
      "258302 973 614 2255\n",
      "255026 917 599 5602\n",
      "256928 1547 465 3204\n",
      "257667 578 388 3511\n",
      "258069 441 384 3250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import ndarray\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "def dice(TP, FP, FN):\n",
    "    return (2*TP)/(FP + (2*TP) + FN)\n",
    "\n",
    "def iou(TP, FP, FN):\n",
    "    return TP/(TP + FP + FN)\n",
    "\n",
    "def ppv(TP, FP):\n",
    "    return TP/(FP + TP)\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "def sensitivity(TP, FN):\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return (TN, FP, FN, TP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_dir = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/test/images/\"\n",
    "mask_dir = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/test/masks/\"\n",
    "\n",
    "img_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))])\n",
    "mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Empty arrays to keep the evaluation results\n",
    "arr_dice = []\n",
    "arr_iou = []\n",
    "arr_ppv = []\n",
    "arr_accuracy = []\n",
    "arr_sensitivity = []\n",
    "MODEL_PATH = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/model_weights/best_swinunet.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = UNet(in_channels=1, num_classes=1).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(img_paths)):\n",
    "\n",
    "      SINGLE_IMG_PATH = img_paths[i]\n",
    "      MASK_PATH = mask_paths[i]\n",
    "\n",
    "      # model = UNet(in_channels=1, num_classes=1).to(device)\n",
    "      # model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n",
    "\n",
    "      transform = transforms.Compose([\n",
    "          transforms.Resize((512, 512)),\n",
    "          transforms.ToTensor()])\n",
    "\n",
    "      img = transform(Image.open(SINGLE_IMG_PATH).convert(\"RGB\")).float().to(device)\n",
    "      mask = transform(Image.open(MASK_PATH).convert(\"L\")).float().to(device)\n",
    "\n",
    "      img = img.unsqueeze(0)\n",
    "      mask = mask.unsqueeze(0)\n",
    "      pred_mask = model(img)\n",
    "\n",
    "      # img = img.squeeze(0).cpu().detach()\n",
    "      # mask = mask.squeeze(0).cpu().detach()\n",
    "      # # mask = mask.cpu().detach()\n",
    "      # img = img.permute(1, 2, 0)\n",
    "      # mask = mask.permute(1, 2, 0)\n",
    "\n",
    "      # pred_mask = pred_mask.squeeze(0).cpu().detach()\n",
    "      # pred_mask = pred_mask.permute(1, 2, 0)\n",
    "      pred_mask[pred_mask <= 0.5]=0\n",
    "      pred_mask[pred_mask > 0.5]=1\n",
    "\n",
    "\n",
    "      # # Binarize the predicted mask\n",
    "      pred_mask = pred_mask.squeeze(0).cpu().detach().numpy() # Remove the batch dimension\n",
    "      mask = mask.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "      mask = mask.flatten()\n",
    "      pred_mask = pred_mask.flatten()\n",
    "    \n",
    "      # print(mask)\n",
    "      TN, FP, FN, TP = perf_measure(mask, pred_mask)\n",
    "\n",
    "\n",
    "\n",
    "      print(TN, FP, FN,  TP)\n",
    "\n",
    "      arr_dice.append(dice(TP, FP, FN))\n",
    "      arr_iou.append(iou(TP, FP, FN))\n",
    "      arr_ppv.append(ppv(TP, FP))\n",
    "      arr_accuracy.append(accuracy(TP, TN, FP, FN))\n",
    "      arr_sensitivity.append(sensitivity(TP, FN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dice:  0.7237057968239025\n",
      "Average iou:  0.5998337245761356\n",
      "Average ppv:  0.723284043482044\n",
      "Average accuracy:  0.9907876480457395\n",
      "Average sensitivity:  0.7600248441450289\n"
     ]
    }
   ],
   "source": [
    "print(\"Average dice: \", np.mean(arr_dice))\n",
    "print(\"Average iou: \", np.mean(arr_iou))\n",
    "print(\"Average ppv: \", np.mean(arr_ppv))\n",
    "print(\"Average accuracy: \", np.mean(arr_accuracy))\n",
    "print(\"Average sensitivity: \", np.mean(arr_sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Noman_venv",
   "language": "python",
   "name": "noman_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
