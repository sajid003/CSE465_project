{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia3SX5dN-LyA"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ngMXAEXF9ExL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ1Fx6xe-P0D"
   },
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1x7eNGKc9R0F"
   },
   "outputs": [],
   "source": [
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # Load grayscale image and mask\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        #img = img.resize((512, 512), Image.BILINEAR)\n",
    "        #mask = mask.resize((512, 512), Image.NEAREST)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        mask = torch.where(mask > 0, 1.0, 0.0)\n",
    "        return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xqBq5MAy9VG2"
   },
   "outputs": [],
   "source": [
    "# 4. Transforms\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),  # This will create 1-channel tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHQXeSmq-a0p"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vv99SK_t9hqI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6292, Validation samples: 1574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change the train images and masks\n",
    "train_images = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/train/images\"\n",
    "train_masks  = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/train/masks\"\n",
    "\n",
    "full_dataset = BrainTumorDataset(train_images, train_masks, transform=transform)\n",
    "\n",
    "# Split: 80% train, 20% validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq0Kq28n-jds"
   },
   "source": [
    "# UNet++ architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "74eJY5CR9jsD"
   },
   "outputs": [],
   "source": [
    "# 5. U-Net++ Model (simplified)\n",
    "# -----------------------------\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1):\n",
    "        super().__init__()\n",
    "        filters = [64, 128, 256, 512]\n",
    "\n",
    "        # Encoder\n",
    "        self.conv0_0 = ConvBlock(in_ch, filters[0])\n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        self.conv1_0 = ConvBlock(filters[0], filters[1])\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2_0 = ConvBlock(filters[1], filters[2])\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3_0 = ConvBlock(filters[2], filters[3])\n",
    "\n",
    "        # Decoder with nested connections\n",
    "        self.up2_1 = nn.ConvTranspose2d(filters[3], filters[2], 2, stride=2)\n",
    "        self.conv2_1 = ConvBlock(filters[2]*2, filters[2])\n",
    "\n",
    "        self.up1_2 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)\n",
    "        self.conv1_2 = ConvBlock(filters[1]*2, filters[1])\n",
    "\n",
    "        self.up0_3 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)\n",
    "        self.conv0_3 = ConvBlock(filters[0]*2, filters[0])\n",
    "\n",
    "        # Final output\n",
    "        self.final = nn.Conv2d(filters[0], out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool0(x0_0))\n",
    "        x2_0 = self.conv2_0(self.pool1(x1_0))\n",
    "        x3_0 = self.conv3_0(self.pool2(x2_0))\n",
    "\n",
    "        # Decoder\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up2_1(x3_0)], dim=1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, self.up1_2(x2_1)], dim=1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, self.up0_3(x1_2)], dim=1))\n",
    "\n",
    "        out = torch.sigmoid(self.final(x0_3))\n",
    "        return out\n",
    "\n",
    "model = UNetPlusPlus().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPurZYHo-pMN"
   },
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8snjQNg99ry-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = 1 - ((2. * intersection + smooth) /\n",
    "                (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
    "    return loss.mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYLEZP6-sa-"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WWSI7yux9t9_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:19<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.6184 | Val Loss: 0.4427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Train Loss: 0.3481 | Val Loss: 0.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Train Loss: 0.3125 | Val Loss: 0.2882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Train Loss: 0.2828 | Val Loss: 0.3059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Train Loss: 0.2666 | Val Loss: 0.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Train Loss: 0.2524 | Val Loss: 0.2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Train Loss: 0.2450 | Val Loss: 0.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Train Loss: 0.2332 | Val Loss: 0.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:19<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Train Loss: 0.2226 | Val Loss: 0.2499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Train Loss: 0.2093 | Val Loss: 0.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Train Loss: 0.2160 | Val Loss: 0.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:19<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Train Loss: 0.1983 | Val Loss: 0.2135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:19<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Train Loss: 0.1919 | Val Loss: 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Train Loss: 0.1864 | Val Loss: 0.2109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Train Loss: 0.1823 | Val Loss: 0.2077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Train Loss: 0.1768 | Val Loss: 0.1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Train Loss: 0.1751 | Val Loss: 0.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Train Loss: 0.1729 | Val Loss: 0.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Train Loss: 0.1637 | Val Loss: 0.1842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 787/787 [01:18<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Train Loss: 0.1583 | Val Loss: 0.1819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 20\n",
    "arr_loss = []\n",
    "arr_val_loss = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        preds = model(images)\n",
    "        loss = dice_loss(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    arr_loss.append(train_loss)\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            preds = model(images)\n",
    "            loss = dice_loss(preds, masks)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    arr_val_loss.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"./best_unetpp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.618406601013495, 0.3481415968667598, 0.3124854265034123, 0.2827809602495524, 0.26660745069289904, 0.25242166413323247, 0.24497065041805918, 0.2332423186305972, 0.22263491756769116, 0.20926080510645081, 0.21596734447279303, 0.1983425939287588, 0.19188927698127803, 0.18637375375480786, 0.18228953460461, 0.17684716698175162, 0.1750772531218874, 0.1729141300996195, 0.16368162277920273, 0.1583376938744603]\n",
      "[0.4427055951756269, 0.35286171337220873, 0.28817695526756004, 0.30594591365247814, 0.2741560292274214, 0.26008611161091605, 0.2410309955023872, 0.26604929146579076, 0.24994958158071875, 0.22167358373476165, 0.2103497960845831, 0.2134701632303635, 0.25144485996913185, 0.21085874331632848, 0.20774140186267456, 0.1987439627347864, 0.1932508124388414, 0.19643148832817367, 0.1842288132473297, 0.18192721835247755]\n"
     ]
    }
   ],
   "source": [
    "print(arr_loss)\n",
    "print(arr_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254975 5780 1181 208\n",
      "254594 5114 2013 423\n",
      "252207 3506 4809 1622\n",
      "252134 5577 2817 1616\n",
      "252880 7723 1501 40\n",
      "248953 7812 4004 1375\n",
      "256083 5224 425 412\n",
      "256572 3707 229 1636\n",
      "247781 4185 10178 0\n",
      "255835 5418 423 468\n",
      "256186 5667 118 173\n",
      "254952 5136 1719 337\n",
      "254878 3167 1389 2710\n",
      "246522 5495 9451 676\n",
      "254093 4373 2721 957\n",
      "249929 8900 2691 624\n",
      "250738 9814 832 760\n",
      "249928 3426 8333 457\n",
      "245714 4937 9124 2369\n",
      "253095 4009 1649 3391\n",
      "254997 3990 1122 2035\n",
      "252974 3998 1874 3298\n",
      "254268 4378 1590 1908\n",
      "254574 1691 3082 2797\n",
      "253197 7782 232 933\n",
      "250750 5134 5214 1046\n",
      "246332 7392 8230 190\n",
      "249454 6873 4348 1469\n",
      "253322 3670 1847 3305\n",
      "256198 4344 429 1173\n",
      "250595 3445 6013 2091\n",
      "247697 5475 6179 2793\n",
      "254276 5194 428 2246\n",
      "255800 3309 2018 1017\n",
      "254033 4556 1525 2030\n",
      "258151 2207 245 1541\n",
      "244624 2290 12940 2290\n",
      "252854 2146 3983 3161\n",
      "250567 6051 4841 685\n",
      "248397 5863 7195 689\n",
      "247110 8361 5251 1422\n",
      "249282 5631 6747 484\n",
      "245479 5141 10761 763\n",
      "254817 4147 400 2780\n",
      "244887 4024 11143 2090\n",
      "244801 4079 10434 2830\n",
      "252056 6869 2967 252\n",
      "251166 7885 1430 1663\n",
      "256012 3956 376 1800\n",
      "256779 4521 151 693\n",
      "245298 6547 8847 1452\n",
      "251419 3168 6516 1041\n",
      "255828 6079 82 155\n",
      "248356 6102 5875 1811\n",
      "248636 5642 5750 2116\n",
      "257834 2902 183 1225\n",
      "253101 8206 837 0\n",
      "257031 3572 701 840\n",
      "244499 11753 5352 540\n",
      "255690 3469 707 2278\n",
      "254436 4573 2217 918\n",
      "250240 4128 7768 8\n",
      "246580 4297 8541 2726\n",
      "254222 3277 1617 3028\n",
      "257971 2753 343 1077\n",
      "252981 2029 3327 3807\n",
      "257909 4000 44 191\n",
      "258804 1707 180 1453\n",
      "252150 6935 2340 719\n",
      "252175 8468 801 700\n",
      "254307 4893 479 2465\n",
      "242600 1115 16210 2219\n",
      "234072 5047 20616 2409\n",
      "255399 3958 906 1881\n",
      "255759 3699 541 2145\n",
      "254491 5452 594 1607\n",
      "258260 1388 865 1631\n",
      "255547 2467 1450 2680\n",
      "253799 3524 2711 2110\n",
      "244866 4882 8965 3431\n",
      "257045 3314 271 1514\n",
      "258150 1709 348 1937\n",
      "253036 6013 2536 559\n",
      "240786 6744 12414 2200\n",
      "255132 6611 103 298\n",
      "257377 1555 420 2792\n",
      "254572 3488 2084 2000\n",
      "250253 5201 6173 517\n",
      "257092 1568 425 3059\n",
      "257695 1811 365 2273\n",
      "249850 3543 6454 2297\n",
      "256198 4344 429 1173\n",
      "257968 1630 1182 1364\n",
      "257829 3146 270 899\n",
      "256418 2885 425 2416\n",
      "257765 3097 624 658\n",
      "242151 1455 15440 3098\n",
      "256129 4302 344 1369\n",
      "258182 3752 59 151\n",
      "257741 2646 568 1189\n",
      "257709 2917 185 1333\n",
      "253978 3417 1404 3345\n",
      "253406 1060 4918 2760\n",
      "255697 1018 2708 2721\n",
      "257533 2996 530 1085\n",
      "247729 4536 8644 1235\n",
      "247735 3275 9775 1359\n",
      "254565 3555 897 3127\n",
      "256095 2806 417 2826\n",
      "255548 984 1146 4466\n",
      "260562 747 112 723\n",
      "256893 4181 363 707\n",
      "256322 2599 603 2620\n",
      "251150 2523 4927 3544\n",
      "255866 4477 408 1393\n",
      "255869 4860 221 1194\n",
      "253890 5252 1348 1654\n",
      "255243 3793 2457 651\n",
      "251929 5644 3424 1147\n",
      "257359 2696 422 1667\n",
      "250905 2888 6572 1779\n",
      "256338 3012 689 2105\n",
      "255469 3419 1919 1337\n",
      "258315 967 740 2122\n",
      "253434 6461 1291 958\n",
      "258424 1330 478 1912\n",
      "257466 2051 326 2301\n",
      "257733 2449 907 1055\n",
      "257557 1928 203 2456\n",
      "252780 2006 2810 4548\n",
      "253091 6605 1925 523\n",
      "254981 4255 2042 866\n",
      "256579 2840 354 2371\n",
      "247916 1110 9159 3959\n",
      "256377 3535 489 1743\n",
      "250293 8231 1707 1913\n",
      "254815 4312 1526 1491\n",
      "251105 3760 2903 4376\n",
      "256260 3935 328 1621\n",
      "255943 5314 815 72\n",
      "254778 3640 2842 884\n",
      "257705 3811 173 455\n",
      "257314 2397 1352 1081\n",
      "258340 3461 90 253\n",
      "256757 2154 1832 1401\n",
      "258059 3439 124 522\n",
      "257030 1537 1174 2403\n",
      "257784 2228 379 1753\n",
      "259802 776 256 1310\n",
      "254568 5308 1319 949\n",
      "254568 3909 1938 1729\n",
      "248825 5641 4550 3128\n",
      "255816 1904 1824 2600\n",
      "255247 1292 3496 2109\n",
      "242142 1770 16353 1879\n",
      "250327 3863 4885 3069\n",
      "244499 1181 11607 4857\n",
      "248229 1351 10541 2023\n",
      "254266 2789 2477 2612\n",
      "253006 1717 3182 4239\n",
      "249613 2251 5852 4428\n",
      "257187 1289 953 2715\n",
      "255546 3621 1185 1792\n",
      "244428 5702 10338 1676\n",
      "245885 3975 9980 2304\n",
      "254314 3311 1648 2871\n",
      "258101 795 469 2779\n",
      "246147 2452 9561 3984\n",
      "239459 9841 11609 1235\n",
      "247302 6645 5848 2349\n",
      "252143 2225 6841 935\n",
      "250956 3873 4854 2461\n",
      "259836 1500 224 584\n",
      "257160 3879 147 958\n",
      "256695 2534 1306 1609\n",
      "259207 1302 1635 0\n",
      "250369 4269 7335 171\n",
      "255559 1174 3996 1415\n",
      "258946 1931 158 1109\n",
      "256086 4676 904 478\n",
      "259968 895 149 1132\n",
      "257942 2001 639 1562\n",
      "254101 1859 3788 2396\n",
      "248122 3193 8643 2186\n",
      "253126 2081 2385 4552\n",
      "250441 5393 5613 697\n",
      "253949 1753 5180 1262\n",
      "256252 3759 1326 807\n",
      "245914 3480 11911 839\n",
      "245815 2578 10849 2902\n",
      "256405 5469 45 225\n",
      "252610 2017 4601 2916\n",
      "230156 4354 26044 1590\n",
      "257078 3816 166 1084\n",
      "257866 2939 190 1149\n",
      "255080 1878 3070 2116\n",
      "257437 2423 644 1640\n",
      "249892 1710 7584 2958\n",
      "249098 2243 9303 1500\n",
      "254518 3360 2475 1791\n",
      "257716 734 947 2747\n",
      "258006 1838 1728 572\n",
      "252637 2725 2502 4280\n",
      "255272 4631 1736 505\n",
      "252879 3946 2928 2391\n",
      "249305 4205 7943 691\n",
      "253476 6284 1940 444\n",
      "250368 1579 6357 3840\n",
      "250002 3148 6642 2352\n",
      "247080 2695 10495 1874\n",
      "250847 1168 7997 2132\n",
      "258183 3158 147 656\n",
      "260278 913 173 780\n",
      "254347 4190 2065 1542\n",
      "257269 1583 587 2705\n",
      "249672 2770 7385 2317\n",
      "253304 3911 4174 755\n",
      "253314 2177 5921 732\n",
      "258429 1090 386 2239\n",
      "247249 6427 6497 1971\n",
      "255460 1568 1655 3461\n",
      "241835 3155 13537 3617\n",
      "255995 4062 344 1743\n",
      "259650 2211 31 252\n",
      "259895 1266 179 804\n",
      "255664 3323 1424 1733\n",
      "259741 1218 196 989\n",
      "259775 1024 279 1066\n",
      "258743 1827 267 1307\n",
      "255856 1269 1338 3681\n",
      "255963 1106 2249 2826\n",
      "259100 1185 415 1444\n",
      "258606 583 381 2574\n",
      "258550 937 230 2427\n",
      "259154 706 450 1834\n",
      "254226 2741 1982 3195\n",
      "257430 2699 403 1612\n",
      "235875 4484 18465 3320\n",
      "238673 1034 18613 3824\n",
      "257104 1702 734 2604\n",
      "255114 2207 1793 3030\n",
      "257101 2838 313 1892\n",
      "252956 2821 1381 4986\n",
      "251593 1808 5721 3022\n",
      "249679 2327 8464 1674\n",
      "248323 3557 7719 2545\n",
      "252668 4315 3388 1773\n",
      "258395 1768 253 1728\n",
      "259106 1257 232 1549\n",
      "254877 4419 354 2494\n",
      "250717 1261 5900 4266\n",
      "244572 1773 11217 4582\n",
      "253100 4092 3688 1264\n",
      "251547 1526 5905 3166\n",
      "239402 7659 13590 1493\n",
      "247722 3691 8174 2557\n",
      "245652 2975 8057 5460\n",
      "245977 4860 9906 1401\n",
      "232263 5708 22450 1723\n",
      "244757 5090 7263 5034\n",
      "240651 8210 13282 1\n",
      "249310 3452 4446 4936\n",
      "250130 4024 6262 1728\n",
      "238007 1522 13491 9124\n",
      "243907 2673 7059 8505\n",
      "247262 2777 9742 2363\n",
      "246379 2030 7560 6175\n",
      "229104 1488 26778 4774\n",
      "245954 3209 9717 3264\n",
      "253674 1919 1449 5102\n",
      "233749 3381 13963 11051\n",
      "230101 2864 18164 11015\n",
      "244482 2778 7549 7335\n",
      "247131 2314 3058 9641\n",
      "249173 3222 4583 5166\n",
      "258067 3015 96 966\n",
      "252885 2167 3008 4084\n",
      "247880 3963 8414 1887\n",
      "238404 2507 20416 817\n",
      "211584 5444 44226 890\n",
      "251036 3698 4542 2868\n",
      "247016 4501 5887 4740\n",
      "247637 3408 5038 6061\n",
      "254521 4746 526 2351\n",
      "240307 5279 14189 2369\n",
      "248468 5288 4091 4297\n",
      "231014 2914 17633 10583\n",
      "247903 5233 7542 1466\n",
      "245223 6554 8461 1906\n",
      "227129 5519 27987 1509\n",
      "244527 4038 7523 6056\n",
      "237643 2729 20796 976\n",
      "253095 3664 2314 3071\n",
      "250284 1673 4353 5834\n",
      "245472 2420 6323 7929\n",
      "246557 6445 7760 1382\n",
      "240002 4991 6386 10765\n",
      "233608 4826 17900 5810\n",
      "237991 1958 14406 7789\n",
      "246509 3425 6436 5774\n",
      "249923 3803 3828 4590\n",
      "257231 3716 484 713\n",
      "251296 4450 3196 3202\n",
      "226859 5240 21668 8377\n",
      "242339 3959 7929 7917\n",
      "232247 7984 21200 713\n",
      "236028 2952 14764 8400\n",
      "235824 8445 13514 4361\n",
      "254226 4012 1740 2166\n",
      "243753 3537 7097 7757\n",
      "250056 5955 3799 2334\n",
      "243884 7072 10082 1106\n",
      "230749 5830 14782 10783\n",
      "240289 7396 14459 0\n",
      "255687 2265 2168 2024\n",
      "243337 3958 10104 4745\n",
      "250741 5225 3345 2833\n",
      "245958 2712 4213 9261\n",
      "252446 4479 3010 2209\n",
      "247233 6350 8561 0\n",
      "240312 4129 15053 2650\n",
      "249996 4172 4309 3667\n",
      "248739 3074 4224 6107\n",
      "255285 5206 142 1511\n",
      "256042 2987 310 2805\n",
      "249578 4231 5978 2357\n",
      "254204 6005 529 1406\n",
      "256376 2108 304 3356\n",
      "255698 5423 474 549\n",
      "256941 1925 269 3009\n",
      "259093 1219 210 1622\n",
      "256056 1175 853 4060\n",
      "252919 4727 2594 1904\n",
      "257184 2724 218 2018\n",
      "252460 7947 367 1370\n",
      "244417 2377 9256 6094\n",
      "257381 1696 973 2094\n",
      "246371 3580 4330 7863\n",
      "249174 5603 2157 5210\n",
      "258484 3116 91 453\n",
      "255160 2143 238 4603\n",
      "249716 1584 4705 6139\n",
      "258219 2250 216 1459\n",
      "254907 2283 854 4100\n",
      "253635 5394 1602 1513\n",
      "255751 1493 528 4372\n",
      "251672 1214 1244 8014\n",
      "246908 4472 7315 3449\n",
      "249924 5028 5972 1220\n",
      "253880 3562 1776 2926\n",
      "249197 3524 4184 5239\n",
      "256015 2364 1840 1925\n",
      "256153 1873 421 3697\n",
      "252499 4497 2957 2191\n",
      "255880 1332 1227 3705\n",
      "244931 3621 7437 6155\n",
      "258565 1222 181 2176\n",
      "256097 2587 371 3089\n",
      "248227 4559 7336 2022\n",
      "254807 1893 490 4954\n",
      "252882 5792 1656 1814\n",
      "255581 4770 136 1657\n",
      "253185 6982 371 1606\n",
      "256399 3171 415 2159\n",
      "259079 1549 108 1408\n",
      "250453 2163 4949 4579\n",
      "258345 1411 232 2156\n",
      "252567 4439 1076 4062\n",
      "258427 2134 112 1471\n",
      "255333 1452 1528 3831\n",
      "258258 1465 421 2000\n",
      "249791 11258 425 670\n",
      "254456 3102 405 4181\n",
      "254695 3852 337 3260\n",
      "256772 2922 508 1942\n",
      "253494 7482 977 191\n",
      "247776 3821 5865 4682\n",
      "255533 3499 948 2164\n",
      "243710 4263 11605 2566\n",
      "254619 2628 1025 3872\n",
      "255318 1633 700 4493\n",
      "255500 3834 612 2198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258239 1954 256 1695\n",
      "257630 1828 260 2426\n",
      "252878 2736 2093 4437\n",
      "258077 1297 557 2213\n",
      "251755 2442 2906 5041\n",
      "253196 4340 1020 3588\n",
      "256082 2434 276 3352\n",
      "255004 5502 254 1384\n",
      "256682 1950 446 3066\n",
      "246786 3187 7632 4539\n",
      "247250 5694 6651 2549\n",
      "244892 3577 12295 1380\n",
      "248145 5135 8861 3\n",
      "243877 4225 11994 2048\n",
      "242361 3628 9457 6698\n",
      "238976 3060 14302 5806\n",
      "230482 4458 23553 3651\n",
      "239718 1606 14563 6257\n",
      "243942 4544 11415 2243\n",
      "245342 3416 10013 3373\n",
      "254261 2764 2690 2429\n",
      "250667 4044 2406 5027\n",
      "255811 2862 411 3060\n",
      "253816 5167 608 2553\n",
      "250511 4040 1839 5754\n",
      "245358 3475 10040 3271\n",
      "246684 3907 2044 9509\n",
      "230097 3229 26144 2674\n",
      "252243 3258 4453 2190\n",
      "253340 5082 2127 1595\n",
      "245983 1029 9030 6102\n",
      "234299 5407 15044 7394\n",
      "241864 5146 11022 4112\n",
      "241895 5062 10991 4196\n",
      "240686 1981 13613 5864\n",
      "248045 5802 8055 242\n",
      "234905 6111 20712 416\n",
      "242329 5590 13080 1145\n",
      "244116 5256 11394 1378\n",
      "252518 1083 4263 4280\n",
      "257457 3077 426 1184\n",
      "256825 807 503 4009\n",
      "254081 1634 281 6148\n",
      "257747 1982 345 2070\n",
      "255876 1398 352 4518\n",
      "255438 3388 1551 1767\n",
      "252882 5792 1656 1814\n",
      "256319 1789 293 3743\n",
      "257626 2706 586 1226\n",
      "258129 1813 303 1899\n",
      "260265 581 172 1126\n",
      "255897 2931 420 2896\n",
      "257091 3103 325 1625\n",
      "256937 1211 950 3046\n",
      "257698 2439 85 1922\n",
      "253640 2921 2141 3442\n",
      "255714 1002 1636 3792\n",
      "258383 2933 162 666\n",
      "255966 2793 306 3079\n",
      "252773 3669 2459 3243\n",
      "258582 802 220 2540\n",
      "257852 3185 187 920\n",
      "249371 1239 6668 4866\n",
      "254472 1076 3820 2776\n",
      "253737 1899 1701 4807\n",
      "256810 1702 1164 2468\n",
      "257073 2613 318 2140\n",
      "253445 1600 3252 3847\n",
      "254680 1003 3001 3460\n",
      "259885 884 135 1240\n",
      "256008 2441 367 3328\n",
      "256413 1608 1350 2773\n",
      "257603 1263 2031 1247\n",
      "257298 3634 125 1087\n",
      "259736 527 194 1687\n",
      "256903 1023 823 3395\n",
      "253902 4023 1482 2737\n",
      "258388 2336 314 1106\n",
      "250408 1562 6401 3773\n",
      "251682 1110 4198 5154\n",
      "258614 521 954 2055\n",
      "250737 2009 3799 5599\n",
      "252949 2174 2651 4370\n",
      "257840 3042 143 1119\n",
      "247753 1975 8807 3609\n",
      "251720 865 3938 5621\n",
      "256682 893 343 4226\n",
      "258373 2254 182 1335\n",
      "254828 4188 1463 1665\n",
      "249366 6488 3034 3256\n",
      "251357 1680 3900 5207\n",
      "254116 1429 1900 4699\n",
      "258825 1305 270 1744\n",
      "257478 1886 181 2599\n",
      "252712 1299 3448 4685\n",
      "249996 2178 6651 3319\n",
      "249139 3632 1369 8004\n",
      "241966 4577 11616 3985\n",
      "249955 3548 3024 5617\n",
      "253112 3729 2018 3285\n",
      "256809 3046 590 1699\n",
      "251766 4372 1780 4226\n",
      "254474 2645 912 4113\n",
      "249648 3756 3176 5564\n",
      "252544 5406 1182 3012\n",
      "241630 6136 14280 98\n",
      "255018 2042 943 4141\n",
      "250130 4122 6327 1565\n",
      "250795 2309 1848 7192\n",
      "246561 3341 6081 6161\n",
      "234684 5144 21949 367\n",
      "255237 1740 321 4846\n",
      "251200 5205 4051 1688\n",
      "254308 1544 3170 3122\n",
      "257262 935 715 3232\n",
      "251894 1378 2248 6624\n",
      "258988 617 387 2152\n",
      "260223 899 180 842\n",
      "256047 1765 217 4115\n",
      "260183 548 107 1306\n",
      "258126 2470 240 1308\n",
      "259330 1643 113 1058\n",
      "256357 1386 2394 2007\n",
      "257684 1372 204 2884\n",
      "257328 1775 419 2622\n",
      "256814 2077 613 2640\n",
      "257767 568 695 3114\n",
      "253017 1477 3250 4400\n",
      "253038 3505 586 5015\n",
      "256272 3282 198 2392\n",
      "256886 4399 123 736\n",
      "256655 1282 302 3905\n",
      "252953 3027 4270 1894\n",
      "256316 1669 1313 2846\n",
      "257884 2162 590 1508\n",
      "249420 1449 3573 7702\n",
      "251582 4861 2056 3645\n",
      "255750 2730 456 3208\n",
      "257687 3677 276 504\n",
      "257635 2255 381 1873\n",
      "257780 2719 115 1530\n",
      "250405 2728 6674 2337\n",
      "249373 2440 6922 3409\n",
      "258049 2157 580 1358\n",
      "259762 841 246 1295\n",
      "255958 1489 1138 3559\n",
      "256435 809 1204 3696\n",
      "258756 725 181 2482\n",
      "256499 1151 826 3668\n",
      "255636 1680 776 4052\n",
      "257534 1418 575 2617\n",
      "253778 3002 3702 1662\n",
      "258937 835 158 2214\n",
      "247966 2165 8249 3764\n",
      "249094 1289 6322 5439\n",
      "252274 6335 1820 1715\n",
      "250500 3844 5331 2469\n",
      "243340 3376 12393 3035\n",
      "251399 4136 4915 1694\n",
      "258028 2605 201 1310\n",
      "249517 7244 2137 3246\n",
      "260300 664 156 1024\n",
      "255438 1199 1293 4214\n",
      "253929 1322 2258 4635\n",
      "252328 1852 2208 5756\n",
      "255529 1579 2287 2749\n",
      "254464 2516 1540 3624\n",
      "257576 1561 818 2189\n",
      "256650 1066 276 4152\n",
      "256794 3867 255 1228\n",
      "256482 938 811 3913\n",
      "251143 1273 3398 6330\n",
      "252917 2681 3906 2640\n",
      "257773 1509 221 2641\n",
      "259553 755 127 1709\n",
      "260447 1156 121 420\n",
      "257843 2523 84 1694\n",
      "253699 1231 1012 6202\n",
      "255179 4832 2119 14\n",
      "259091 1430 141 1482\n",
      "259558 2284 51 251\n",
      "255619 5096 344 1085\n",
      "254903 5681 1560 0\n",
      "258546 2951 112 535\n",
      "256998 2439 1638 1069\n",
      "260058 578 138 1370\n",
      "256765 2375 821 2183\n",
      "251493 8486 1267 898\n",
      "259478 1930 93 643\n",
      "256631 4349 144 1020\n",
      "258077 2677 1375 15\n",
      "257767 2702 569 1106\n",
      "257310 3477 192 1165\n",
      "259605 2142 50 347\n",
      "251631 5775 3457 1281\n",
      "253565 1543 2170 4866\n",
      "253945 4435 2679 1085\n",
      "258913 1595 282 1354\n",
      "260405 724 122 893\n",
      "252631 6489 2121 903\n",
      "258004 3135 1002 3\n",
      "255386 4629 1278 851\n",
      "259453 1262 160 1269\n",
      "256170 3520 1429 1025\n",
      "255743 4270 309 1822\n",
      "259376 1859 216 693\n",
      "257147 3792 312 893\n",
      "255611 5065 865 603\n",
      "257243 3560 420 921\n",
      "257908 3138 127 971\n",
      "258480 2286 199 1179\n",
      "256985 1640 446 3073\n",
      "255067 3388 621 3068\n",
      "256162 4401 193 1388\n",
      "258150 1668 432 1894\n",
      "253731 4035 2745 1633\n",
      "255739 4460 869 1076\n",
      "252449 5731 3417 547\n",
      "255652 4925 1249 318\n",
      "256526 4009 288 1321\n",
      "256071 2506 788 2779\n",
      "254098 5647 1592 807\n",
      "257646 2781 409 1308\n",
      "255728 4282 1406 728\n",
      "255128 6331 682 3\n",
      "255476 4344 1870 454\n",
      "258154 2092 313 1585\n",
      "257717 2556 1819 52\n",
      "255102 3387 1563 2092\n",
      "259398 1225 226 1295\n",
      "255537 5098 998 511\n",
      "257238 3765 348 793\n",
      "253167 3630 3471 1876\n",
      "254920 4420 2407 397\n",
      "253918 4251 1448 2527\n",
      "256733 3619 470 1322\n",
      "259736 1102 211 1095\n",
      "255112 3215 928 2889\n",
      "254257 5657 2202 28\n",
      "257415 2775 1281 673\n",
      "257402 3145 356 1241\n",
      "254621 5333 1820 370\n",
      "252229 9216 656 43\n",
      "255449 4701 1968 26\n",
      "254837 6320 618 369\n",
      "257110 3422 1152 460\n",
      "256444 2949 1687 1064\n",
      "259030 2504 78 532\n",
      "257022 4059 226 837\n",
      "255018 4185 1768 1173\n",
      "253352 4323 4081 388\n",
      "255315 5218 1226 385\n",
      "255553 5024 594 973\n",
      "254417 4092 376 3259\n",
      "253286 7596 533 729\n",
      "253293 7100 1448 303\n",
      "253721 6464 1949 10\n",
      "255195 4948 688 1313\n",
      "255294 4774 2076 0\n",
      "256202 4766 862 314\n",
      "256586 4778 341 439\n",
      "256538 2919 2337 350\n",
      "259345 1421 134 1244\n",
      "259720 632 178 1614\n",
      "253552 6182 1633 777\n",
      "259646 1724 110 664\n",
      "259621 2097 83 343\n",
      "259883 1071 79 1111\n",
      "255375 5070 587 1112\n",
      "257821 2509 233 1581\n",
      "256313 4793 443 595\n",
      "259410 1626 75 1033\n",
      "260826 743 102 473\n",
      "254630 4734 1184 1596\n",
      "251269 5931 3698 1246\n",
      "253740 1209 1603 5592\n",
      "255664 4816 261 1403\n",
      "254937 4210 2229 768\n",
      "254948 3323 2426 1447\n",
      "255805 5156 468 715\n",
      "258793 1599 253 1499\n",
      "261091 365 146 542\n",
      "254775 6508 93 768\n",
      "257325 3828 598 393\n",
      "256356 3460 1366 962\n",
      "257828 2611 229 1476\n",
      "251859 5692 2966 1627\n",
      "256322 3345 2095 382\n",
      "250432 9349 1488 875\n",
      "255453 5831 142 718\n",
      "255468 4806 1351 519\n",
      "258473 2615 149 907\n",
      "256027 5468 243 406\n",
      "257222 2773 1001 1148\n",
      "258553 2371 182 1038\n",
      "259337 1470 161 1176\n",
      "255377 3549 340 2878\n",
      "258629 2873 20 622\n",
      "256735 2740 146 2523\n",
      "251732 8515 293 1604\n",
      "258311 2415 171 1247\n",
      "258937 1724 268 1215\n",
      "257808 779 718 2839\n",
      "257820 1986 269 2069\n",
      "258584 2112 934 514\n",
      "253714 2072 5047 1311\n",
      "257507 754 1802 2081\n",
      "257765 3327 320 732\n",
      "259020 2103 172 849\n",
      "256042 4157 1193 752\n",
      "255601 4138 2397 8\n",
      "256026 687 4170 1261\n",
      "257249 2033 2195 667\n",
      "253027 4309 3586 1222\n",
      "256351 2456 2121 1216\n",
      "257907 4056 26 155\n",
      "255217 4531 2381 15\n",
      "255623 2692 3146 683\n",
      "255075 2711 3057 1301\n",
      "255942 4043 2158 1\n",
      "256780 3133 1990 241\n",
      "257493 2156 1281 1214\n",
      "257042 2674 715 1713\n",
      "257536 2559 170 1879\n",
      "259237 994 237 1676\n",
      "255328 4094 1706 1016\n",
      "258204 1262 1527 1151\n",
      "255442 4905 1175 622\n",
      "257629 1159 1184 2172\n",
      "257558 1665 902 2019\n",
      "254933 3791 1050 2370\n",
      "254085 4530 3375 154\n",
      "258932 1675 500 1037\n",
      "255496 1025 3342 2281\n",
      "258515 2209 80 1340\n",
      "257496 2816 453 1379\n",
      "250901 2943 7606 694\n",
      "256421 1347 2749 1627\n",
      "256396 1941 426 3381\n",
      "256915 2012 2598 619\n",
      "259153 1218 976 797\n",
      "256503 3694 1115 832\n",
      "257079 3138 790 1137\n",
      "255397 4238 2508 1\n",
      "254307 6643 932 262\n",
      "255367 4401 1442 934\n",
      "256293 2154 2341 1356\n",
      "255397 2160 2105 2482\n",
      "254116 1213 3500 3315\n",
      "254892 2377 3506 1369\n",
      "252982 3478 2712 2972\n",
      "259185 965 636 1358\n",
      "258330 1335 221 2258\n",
      "255145 5964 1033 2\n",
      "254988 2108 1895 3153\n",
      "251879 6739 3519 7\n",
      "259166 1392 828 758\n",
      "257651 2450 1833 210\n",
      "248393 904 6508 6339\n",
      "253488 4923 3733 0\n",
      "255608 2957 1514 2065\n",
      "254368 2291 3341 2144\n",
      "257152 1474 2855 663\n",
      "255742 5810 592 0\n",
      "258465 1759 167 1753\n",
      "254209 7173 762 0\n",
      "255085 5685 1271 103\n",
      "258278 3163 152 551\n",
      "256589 1914 1237 2404\n",
      "254682 5108 2147 207\n",
      "253389 4375 3725 655\n",
      "254332 972 5232 1608\n",
      "258280 2050 599 1215\n",
      "255989 1751 2059 2345\n",
      "259193 1203 537 1211\n",
      "258906 531 970 1737\n",
      "256883 3957 580 724\n",
      "258521 1799 491 1333\n",
      "257855 501 1743 2045\n",
      "259619 1343 180 1002\n",
      "259615 1781 141 607\n",
      "252367 1006 4046 4725\n",
      "256914 1753 2142 1335\n",
      "258129 956 1209 1850\n",
      "253238 2421 4760 1725\n",
      "254688 2766 2853 1837\n",
      "259487 1118 391 1148\n",
      "257321 2480 2264 79\n",
      "254737 4771 2613 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258321 2407 306 1110\n",
      "252191 5573 3385 995\n",
      "255245 3268 2557 1074\n",
      "257964 2926 343 911\n",
      "257291 2848 1513 492\n",
      "255905 4046 1058 1135\n",
      "257140 4392 232 380\n",
      "256759 3969 509 907\n",
      "253240 5327 556 3021\n",
      "255731 2361 2525 1527\n",
      "251437 5455 4504 748\n",
      "258811 837 872 1624\n",
      "259069 2056 146 873\n",
      "254649 2083 2046 3366\n",
      "257245 3581 602 716\n",
      "254987 4714 1735 708\n",
      "256548 3806 1666 124\n",
      "254465 6978 566 135\n",
      "254413 5750 1964 17\n",
      "254716 4383 1859 1186\n",
      "258486 1879 488 1291\n",
      "254061 6544 1158 381\n",
      "254663 4295 3186 0\n",
      "255086 5575 1467 16\n",
      "257651 3805 684 4\n",
      "257749 1130 1041 2224\n",
      "255192 5240 1712 0\n",
      "254866 5611 851 816\n",
      "257433 3459 390 862\n",
      "254992 5417 973 762\n",
      "257092 1631 859 2562\n",
      "256861 3245 1373 665\n",
      "259229 1601 244 1070\n",
      "258211 2521 295 1117\n",
      "259388 1680 125 951\n",
      "257118 4228 158 640\n",
      "254467 6144 1458 75\n",
      "253370 5295 660 2819\n",
      "254564 3588 2737 1255\n",
      "253376 6297 2471 0\n",
      "257112 3663 424 945\n",
      "257222 2856 424 1642\n",
      "258930 2561 87 566\n",
      "258008 3411 136 589\n",
      "258628 2802 317 397\n",
      "258272 2911 961 0\n",
      "257141 3319 1684 0\n",
      "254344 6054 1746 0\n",
      "253812 7321 1011 0\n",
      "257548 1271 2474 851\n",
      "254815 3545 3518 266\n",
      "253597 5112 2556 879\n",
      "254875 5375 699 1195\n",
      "256795 3625 324 1400\n",
      "255739 4731 1674 0\n",
      "255775 4296 457 1616\n",
      "257210 709 2843 1382\n",
      "258585 444 2319 796\n",
      "252165 5394 4514 71\n",
      "257505 3403 338 898\n",
      "256105 2756 1453 1830\n",
      "247040 2062 8930 4112\n",
      "258485 699 351 2609\n",
      "252765 4307 4305 767\n",
      "256102 4671 1371 0\n",
      "258158 2244 193 1549\n",
      "259465 1600 162 917\n",
      "259320 2086 132 606\n",
      "258403 3289 61 391\n",
      "258209 3064 871 0\n",
      "256490 5102 185 367\n",
      "256513 1574 1821 2236\n",
      "254287 4099 3493 265\n",
      "253159 3901 5084 0\n",
      "256235 5218 153 538\n",
      "255595 5263 654 632\n",
      "259512 2211 43 378\n",
      "258884 2411 382 467\n",
      "258450 1371 689 1634\n",
      "257078 4483 90 493\n",
      "258024 3696 422 2\n",
      "256081 2634 1835 1594\n",
      "257670 3403 516 555\n",
      "257043 2707 1343 1051\n",
      "255694 3384 2001 1065\n",
      "251892 3971 1905 4376\n",
      "256335 1783 2211 1815\n",
      "256290 1725 2267 1862\n",
      "257485 857 2210 1592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import ndarray\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "def dice(TP, FP, FN):\n",
    "    return (2*TP)/(FP + (2*TP) + FN)\n",
    "\n",
    "def iou(TP, FP, FN):\n",
    "    return TP/(TP + FP + FN)\n",
    "\n",
    "def ppv(TP, FP):\n",
    "    return TP/(FP + TP)\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "def sensitivity(TP, FN):\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return (TN, FP, FN, TP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_dir = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/test/images/\"\n",
    "mask_dir = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/dataset/segmentation_task/test/masks/\"\n",
    "\n",
    "img_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))])\n",
    "mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Empty arrays to keep the evaluation results\n",
    "arr_dice = []\n",
    "arr_iou = []\n",
    "arr_ppv = []\n",
    "arr_accuracy = []\n",
    "arr_sensitivity = []\n",
    "MODEL_PATH = \"/home/readinggroup/Desktop/Image_proc_Noman/CSE465_project/model_weights/best_unetplusplus.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = UNetPlusPlus().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(img_paths)):\n",
    "\n",
    "      SINGLE_IMG_PATH = img_paths[i]\n",
    "      MASK_PATH = mask_paths[i]\n",
    "\n",
    "      # model = UNet(in_channels=1, num_classes=1).to(device)\n",
    "      # model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n",
    "\n",
    "      transform = transforms.Compose([\n",
    "          transforms.Resize((512, 512)),\n",
    "          transforms.ToTensor()])\n",
    "\n",
    "      img = transform(Image.open(SINGLE_IMG_PATH).convert(\"L\")).float().to(device)\n",
    "      mask = transform(Image.open(MASK_PATH).convert(\"L\")).float().to(device)\n",
    "\n",
    "      img = img.unsqueeze(0)\n",
    "      mask = mask.unsqueeze(0)\n",
    "      pred_mask = model(img)\n",
    "\n",
    "      # img = img.squeeze(0).cpu().detach()\n",
    "      # mask = mask.squeeze(0).cpu().detach()\n",
    "      # # mask = mask.cpu().detach()\n",
    "      # img = img.permute(1, 2, 0)\n",
    "      # mask = mask.permute(1, 2, 0)\n",
    "\n",
    "      # pred_mask = pred_mask.squeeze(0).cpu().detach()\n",
    "      # pred_mask = pred_mask.permute(1, 2, 0)\n",
    "      pred_mask[pred_mask <= 0.5]=0\n",
    "      pred_mask[pred_mask > 0.5]=1\n",
    "\n",
    "\n",
    "      # # Binarize the predicted mask\n",
    "      pred_mask = pred_mask.squeeze(0).cpu().detach().numpy() # Remove the batch dimension\n",
    "      mask = mask.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "      mask = mask.flatten()\n",
    "      pred_mask = pred_mask.flatten()\n",
    "    \n",
    "      # print(mask)\n",
    "      TN, FP, FN, TP = perf_measure(mask, pred_mask)\n",
    "\n",
    "\n",
    "\n",
    "      print(TN, FP, FN, TP)\n",
    "\n",
    "      arr_dice.append(dice(TP, FP, FN))\n",
    "      arr_iou.append(iou(TP, FP, FN))\n",
    "      arr_ppv.append(ppv(TP, FP))\n",
    "      arr_accuracy.append(accuracy(TP, TN, FP, FN))\n",
    "      arr_sensitivity.append(sensitivity(TP, FN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dice:  0.4044408815246971\n",
      "Average iou:  0.28023155874891237\n",
      "Average ppv:  0.3843992194679883\n",
      "Average accuracy:  0.97532041239184\n",
      "Average sensitivity:  0.5288162957616644\n"
     ]
    }
   ],
   "source": [
    "print(\"Average dice: \", np.mean(arr_dice))\n",
    "print(\"Average iou: \", np.mean(arr_iou))\n",
    "print(\"Average ppv: \", np.mean(arr_ppv))\n",
    "print(\"Average accuracy: \", np.mean(arr_accuracy))\n",
    "print(\"Average sensitivity: \", np.mean(arr_sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Noman_venv",
   "language": "python",
   "name": "noman_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
